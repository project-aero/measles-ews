---
title: "Supporting Information for:"
subtitle: "Critical slowing down anticipates emergence and elimination of measles"
author: "Andrew T. Tredennick, Eamon O'Dea, TBD, Pejman Rohani, John M. Drake"
output: 
  pdf_document:
    number_sections: true
    toc_depth: 2
    keep_tex: true
    keep_md: true
date: "`r Sys.Date()`"
csl: proceedings-of-the-royal-society-b.csl
bibliography: ./measles-ews.bib
header-includes:
  - \renewcommand{\thefootnote}{\fnsymbol{footnote}}
  - \renewcommand{\theequation}{S.\arabic{equation}}
  - \renewcommand{\thetable}{S\arabic{table}}
  - \renewcommand{\thefigure}{S\arabic{figure}}
  - \renewcommand{\thesection}{Section S\arabic{section}}
  - \usepackage{float}
  - \floatplacement{figure}{H}
  - \usepackage{amsmath,amssymb}
  - \usepackage{mathptmx}
  - \newcommand{\comment}[1]{\ignorespaces}
---

```{r libraries, include=FALSE, message=FALSE, warning=TRUE}
knitr::opts_chunk$set(echo=FALSE, message=FALSE, warning=TRUE, cache=FALSE)
library(tidyverse)
library(lubridate)
library(stringr)
library(dplyr)
library(pomp)
library(ggthemes)
library(cowplot)
```

# Demographic data used in model fitting
We used two sources of demographic data to help constrain model fitting: total population size (*N*) and birth rate ($\mu$).
Both sets of data are reported on an annual basis.
To incorporate the data in our model, we fit splines through the annual observations to interpolate daily values (Fig. \ref{demog-plot}).
Note that the daily interpolated birth rate is still in units of "per year," which is why births are modeled as $\mu N \times dt$ to include the effect of the daily time step *dt*.

```{r demo-data, fig.width=8.5, fig.height=3, fig.cap="Plots of the demographic data used for model fitting. The points are the actual data and the lines are the interpolated spline fits. Population data is at the disrict level, meaning we have different data for each focal city. Birth rate data is national, meaning each city shares the same birth rates. \\label{demog-plot}"}

annual_pop <- read_csv("../data/raw-data/district_pops_colnames.csv",
                       col_types = cols(
  region = col_character(),
  `1995` = col_double(),
  `1996` = col_double(),
  `1997` = col_double(),
  `1998` = col_double(),
  `1999` = col_double(),
  `2000` = col_double(),
  `2001` = col_double(),
  `2002` = col_double(),
  `2003` = col_double(),
  `2004` = col_double(),
  `2005` = col_double()
))
annual_births <- read_csv("../data/raw-data/niger_crude_birth_rates.csv",
                          col_types = cols(
  date = col_character(),
  births_per_thousand = col_double()
))
demog_clean <- readRDS("../data/clean-data/annual-demographic-data-niger-cities-clean.RDS")

# Births
birth_data <- annual_births %>%
  mutate(
    date = mdy(date),  # lubridate prefixes any 2digit year 00-68 with 20, 
                       # not a problem for us though
    year = as.character(year(date)),
    rate_per_person_per_year = births_per_thousand/1000
  ) %>%
  dplyr::select(year, rate_per_person_per_year) %>%
  mutate(year = as.numeric(year)) %>%
  filter(year > 1994 & year < 2006) %>%
  mutate(time = year+0.000)

ggplot() +
  geom_line(data = filter(demog_clean, time <= 2005), aes(x = time, y = birth_per_person_per_year), color = "grey35") +
  geom_point(data = birth_data, aes(x = time, y = rate_per_person_per_year), color = "grey35") +
  labs(x = "Time", y = expression(paste("Annual per capita birth rate (", mu, ")"))) +
  scale_color_colorblind(name = NULL) +
  theme_minimal() -> birth_plot

# Population
pop_data <- annual_pop %>%
  gather(key = year, value = population, -region) %>%
  mutate(
    region = ifelse(region == "Niamey I", "Niamey", region),
    year = as.numeric(year),
    time = year + 0.000  # make time double
  ) %>%
  filter(region %in% c("Agadez", "Maradi", "Niamey", "Zinder"))

demog_clean <- demog_clean %>%
  mutate(
    region = ifelse(region == "Agadez (City)", "Agadez", region),
    region = ifelse(region == "Maradi (City)", "Maradi", region),
    region = ifelse(region == "Niamey (City)", "Niamey", region),
    region = ifelse(region == "Zinder (City)", "Zinder", region)
  )

ggplot() +
  geom_line(data = filter(demog_clean, time <= 2005), aes(x = time, y = population_size/1000, color = region)) +
  geom_point(data = pop_data, aes(x = time, y = population/1000, color = region)) +
  labs(x = "Time", y = expression(paste("Population size, ", italic(N), " (1000s)"))) +
  scale_color_colorblind(name = NULL) +
  theme_minimal() +
  theme(legend.position = c(0.2, 0.9),
        legend.key.size = unit(1, 'lines')) -> pop_plot

plot_grid(birth_plot, pop_plot, nrow = 1, align = "h")
```

# Stochastic simulations from the fitted models

Here we show stochastic simulations from the fitted models (Fig. \ref{stoch-sims}).
Simulations are all initialized from the same initial conditions, which were estimated as part of model fitting.

```{r stoch-sims, fig.cap="Stochastic simulations from the fitted models. Each colored line is a single realization of the model from the same initial conditions. Ten simulations are shown. The black lines are the observed data. We do not expect model simulations to perfectly align with the data because of the multiple sources of stochasticity present. Thus, these simulations should be judged by their ability to reproduce dynamical features of the data. \\label{stoch-sims}", fig.width=8.5, fig.height=4}

code_files <- list.files("../code/")
pomp_ids <- grep("measles-pomp-object", code_files)
pomp_files <- code_files[pomp_ids]

results_files <- list.files("../results/")
mle_ids <- grep("mif-lls", results_files)
mle_files <- results_files[mle_ids]
boot_ids <- grep("boot", mle_files)
mle_files <- mle_files[-boot_ids]

source("../code/make-legacy-pomp-filtering-function.R")

all_sims <- tibble()
for(do_file in pomp_files){
  tmp_city <- str_sub(do_file, start = 21, end = nchar(do_file)-4)
  tmp_pomp <- readRDS(paste0("../code/", do_file))
  

  
  city_mle_ids <- grep(tmp_city, mle_files)
  tmp_mles <- read.csv(paste0("../results/", mle_files[city_mle_ids])) %>%
    drop_na() %>%
    filter(loglik == max(loglik)) %>%
    dplyr::select(-loglik, -loglik_se, -do_grid)

  obs_data <- data.frame(time = tmp_pomp@times, reports = as.numeric(tmp_pomp@data))
  covar_data <- cbind(time = tmp_pomp@tcovar, as.data.frame(tmp_pomp@covar))
  
  sim <- make_pomp_filter(obs_data = obs_data, 
                          covar_data = covar_data, 
                          mles = unlist(tmp_mles))
  
  tmp_sim <- pomp::simulate(
    sim,
    params = unlist(tmp_mles),
    nsim = 10,
    as.data.frame = TRUE,
    include.data = TRUE
  ) %>%
    dplyr::select(time, sim, reports, S) %>%
    mutate(city = tmp_city) %>%
    filter(time >= 1995)
  
  all_sims <- bind_rows(all_sims, tmp_sim)
}

ggplot() +
  geom_line(data = filter(all_sims, sim != "data"), aes(x = time, y = reports, color = sim), alpha = 0.25) +
  geom_line(data = filter(all_sims, sim == "data"), aes(x = time, y = reports)) +
  guides(color = FALSE) +
  facet_wrap(~city, scales = "free_y") +
  labs(x = "Date", y = expression(sqrt(Reports))) +
  scale_y_sqrt() +
  theme_minimal()
```

# Comparisons with benchmarking models
Here we compare our fitted *SEIR* models to two non-mechanistic benchmarking models.
Doing so allows us to gain some intuition as to whether accounting for mechanism (i.e., transmission dynamics) improves model fit and eventual inference.
The first benchmarking model is a negative binomial sampling model (NBM) that simply assumes all observations are independent and identically distributed.
The second benchmarking model is a seasonal moving average model (SARIMA) that can account for data dependencies and seasonal periodicity.
Beating the SARIMA is a harder test than beating the NBM.

We fitted both models to the weekly case count observations for each city using maximum likelihood.
After fitting, we calculated Akaike's Information Criterion for each model as: $\text{AIC} = 2k -2\text{ln}(L)$, where *k* is the number of estimated parameters in the model and *L* is the model's likelihood.
The NBM model has 2 parameters, the SARIMA model has 7 parameters, and the SEIR model has 14 parameters.
The results in table S1 show the AIC values for each model.
\comment{I think the SARIMA model should be described briefly for the sake of reproducibility.}
The AIC values for the *SEIR* models are lowest for all cities, suggesting that the addition of mechanism via explicit disease transmission dynamics substantially improves model fit.

```{r aic-comp}
data_file <- "../data/clean-data/weekly-measles-incidence-niger-cities-clean.RDS"
measles <- readRDS(data_file) %>%
  filter(year > 1994)

nb_lik <- function(theta) {
  -sum(dnbinom(as.vector(cases),size=exp(theta[1]),prob=exp(theta[2]),log=TRUE))
} 

all_logliks <- tibble()
for(do_city in unique(measles$region)){
  cases <- measles %>%
    filter(region == do_city) %>%
    pull(cases)
  
  # Negative binomial iid sampling model
  nb_mle <- optim(c(0,-5), nb_lik)
  nb_nll <- -nb_mle$value
  
  # SARIMA model
  log_y <- log(as.vector(cases)+1)
  arma_fit <- arima(log_y,order=c(2,0,2),seasonal=list(order=c(1,0,1),period=52), method = "CSS")
  arma_nll <- arma_fit$loglik-sum(log_y)
  
  # SEIR fit log likelihood
  mif_city <- str_sub(do_city, 1, 6)
  seir_nll <- read.csv(file = paste0("../results/initial-mif-lls-", mif_city, ".csv")) %>%
    drop_na() %>%
    filter(loglik == max(loglik)) %>%
    pull(loglik)
  
  # Store results
  all_logliks %>%
    bind_rows(
      tibble(
        City = mif_city,
        `Neg. Binomial` = nb_nll,
        SARIMA = arma_nll,
        SEIR = seir_nll
      )
    ) -> all_logliks
}

num_params <- tibble(
  `Neg. Binomial` = 2,
  SARIMA = 7,
  SEIR = 14
) %>%
  gather(model, k)

all_logliks %>%
  gather(model, loglik, -City) %>%
  left_join(num_params, by = "model") %>%
  mutate(
    AIC = round(2*k - 2*loglik)
  ) %>%
  dplyr::select(-loglik, -k) %>%
  spread(key = model, value = AIC) %>%
  knitr::kable(format = "pandoc", caption = "AIC values for the benchmarking and SEIR models.")
```

# Long-run simulations from the fitted the SEIR models
For fitting the SEIR model, we used known population size interpolated between years.
This meant we were able to ignore certain demographic processes.
For example, we ignored deaths from the susceptible pool under the assumption that the infection rate was much faster than death rate.
We also ignored the recovered class completely because their dynamics, outside of the contribution to population size (which we assumed to be known), do not impact the $S, E, \text{or } I$ compartments.
However, we needed to include births and deaths from all compartments, including $R$, when simulating the model over arbitrarily long time periods that do not necessarily represent real times for which we would have information on population size.

We added a few terms to the *SEIR* model presented in the main text to account for births and deaths in all compartments.
Our strategy was to simulate a population at a dynamic equilibrium, where total population was neither increasing or decreasing over time, but it does fluctuate.
Our *simulating model* (in contrast to the *fitting model* in the main text) is as follows.

As in the main text, the *SEIR* model is specified as a set of difference equations,

\begin{align}
S_{t+dt} - S_{t} &= n_{0S,t} - n_{SE,t} - n_{S0,t} \\
E_{t+dt} - E_{t} &= n_{SE,t} - n_{EI,t} - n_{E0,t} \\
I_{t+dt} - I_{t} &= n_{EI,t} + n_{0I,t} - n_{IR,t} - n_{I0,t} \\
R_{t+dt} - R_{t} &= n_{0R,t} + n_{IR,t} - n_{R0,t},
\end{align}

\noindent{}where $\textbf{n}_t$ are random variables representing the number of individuals transitioning into or out of each class at each update $t \rightarrow t+dt$.
Transition definitions are in Table S2.

| Random variable | Transition | $\left(\Delta S, \Delta E, \Delta I, \Delta R\right)$ |
| ---------- | ----------------------------| ---------- |
| $n_{0S}$ | Births into the *S* compartment, not vaccinated | (1, 0, 0, 0) |
| $n_{SE}$ | Number of people transitioning from *S* to *E* | (-1, 1, 0, 0) |
| $n_{SO}$ | Number of deaths leaving *S* | (-1, 0, 0, 0) |
| $n_{EI}$ | Number of people transitioning from *E* to *I* | (0, -1, 1, 0) |
| $n_{E0}$ | Number of deaths leaving *E* | (0, -1, 0, 0) |
| $n_{0I}$ | Number of imported infections | (0, 0, 1, 0) |
| $n_{IR}$ | Number of people transitioning from *I* to *R* | (0, 0, -1, 1) |
| $n_{I0}$ | Number of deaths leaving *I* | (0, 0, -1, 0) |
| $n_{0R}$ | Births into the *R* compartment, vaccinated | (0, 0, 0, 1) |
| $n_{R0}$ | Number of deaths leaving *R* | (0, 0, 0, -1) |

Table: Transitions in the simulating model.

The stochastic random variables are specified as follows:

\begin{align}
n_{0S,t} &\sim \text{Poisson}((1-p)\mu N_t \times dt) \\
(n_{SE,t}, n_{S0,t}) &\sim \text{Mult}(S_{t}, (\lambda_{SE,t}, \lambda_{d})) \\
(n_{EI,t}, n_{E0,t}) &\sim \text{Mult}(E_{t}, (\lambda_{EI,t}, \lambda_{d})) \\
(n_{IR,t}, n_{I0,t}) &\sim \text{Mult}(I_{t}, (\lambda_{IR,t}, \lambda_{d})) \\
n_{I0,t} &\sim \text{Poisson}(\psi \times dt) \\
n_{0R,t} &\sim \text{Poisson}(p\mu N_t \times dt) \\
n_{R0,t} &\sim \text{Binomial}(R_t, \lambda_{d,t}),
\end{align}

\noindent{}where *p* is the vaccination coverage (set at $p = 0.7$), $\mu$ is the constant birth rate (set at $\mu = 0.05$), $\psi$ is the rate of imported infections (estimated by the model), and $\lambda_d$, $\lambda_E$, $\lambda_I$, and $\lambda_R$ are the probabilities of death, exposure, becoming infectious, and recovery, respectively.
These probabilities are modeled as:

\begin{align}
\lambda_{SE,t} &= 1 - e^{-\frac{\beta_t I_t dt}{N_t}} \\
\lambda_{EI,t} &= 1 - e^{-\eta E_{t} dt} \\
\lambda_{IR,t} &= 1 - e^{-\gamma I_{t} dt} \\
\lambda_{d,t} &= 1 - e^{-\nu N_t dt}
\end{align}

\noindent{}where $\beta_t$ is a time-varying rate of transmission, $\eta$ is a time-invariant rate from the exposed class to the infectious class, $\gamma$ is a time-invariant recovery rate, and $\nu$ is the constant death rate.
We set $1/\eta = 8$ days, $1/\gamma = 5$ days, and $\nu = \mu = 0.05$. We used a daily time step of $dt = \text{year} / 365$.
Recall that we set birth rate equal to death rate ($\nu = \mu = 0.05$) to achieve a relatively constant equilibrium population size.
We modeled the rate of transmission as:

\begin{equation}
\beta_t = \beta \left(1 + \exp \left(\sum^6_{i=1} q_i \xi_{i_{t}}\right) \right) \Gamma_t.
\end{equation}

$\beta$ is the mean transmission rate and the term $\sum^6_{i=1} q_i \xi_{i_{t}}$ is a B-spline to model seasonality in transmission.
The B-spline bases ($\xi_{i_{t}}$) are periodic with a 1 year period.
The transmission rate ($\beta_t$) is also subject to stochastic process noise at each time step, $\Gamma_t$, which we model as gamma-distributed white (temporally uncorrelated) noise with mean 1 and intensity $\sigma^2$ [@Breto2011].

\comment{I think the size of the process time step needs to be stated somewhere. Was it 1 day?}

Finally, observations were simulated from the model on a weekly time step by drawing counts of cases ($y_t$) from a negative binomial distribution centered on the number of people that transitioned from the infectious to recovered class over seven days ($x_t$), subject to estimated reporting rate ($\rho$), and with a dispersion parameter equal to the MLE: $y_t \sim \text{Negative binomial}(\rho x_t, \tau)$.

In this model, the time-average of the basic reproduction number can be calculated as:

$$
R_0 = \frac{\eta \beta \mu}{\nu (\eta + \nu)(\gamma + \nu)}.
$$

The model described above was implemented in the `R` package `pomp` [@King2016; @King2018] to simulate the re-emergence and elimination scenarios.

# Computing the time of critical transitions
To define the null and test intervals for our simulations of re-emergence and elimination, we need to know when the critical transition between alternative modes of fluctuation occurs.
For re-emergence, we defined the year of the critical transition as the year just after the effective reproduction number ($R_E$) reaches or exceeds the critical value of 1. 
For example, if $R_E$ reaches or exceeds 1 at some point during the fifth year of the simulation, then the critical transition year is defined as the sixth year of the simulation. 
Thus, the simulated data for calculating early warning signals ends at the end of the fifth year.
We call this year the ``critical year.''
From this full window, from the beginning of the simulation to the end of the critical year, we defined the null interval as the first half of the window (far from $R_E = 1$) and the test interval as the second half of the full window (near $R_E = 1$).
Figure \ref{emerge-sim} shows a typical example.

```{r emerge-windows, fig.width=6, fig.cap="A typical emergence simulation for Maradi where the initial number of susceptibles was discounted by 1e-04. (**A**) The simulated trajectory of $R_E$ and the year in which $R_E$ first reaches the critical value of 1 (denoted by dashed blue line). (**B**) The simulated trajectory of the number of cases. Note that the x-axis has been reduced relative to the top panel. The two vertical blue lines indicate the start (left-most line) and end (line for critical year) of the full window. The black line demarcates the division between the equal-length null and test intervals. \\label{emerge-sim}"}
ex_file <- "../simulations/emergence-simulations-grid-Maradi-1e-04.RDS"
emerge_sim <- readRDS(ex_file)

re_one_year <- emerge_sim %>%
  group_by(time) %>%
  summarise(mean_re = mean(RE_seas)) %>%  
  mutate(year = round(time)) %>%  # create 'year' variable
  ungroup() %>%
  filter(mean_re >= 1) %>%  # drop times where Re less than 1
  filter(year == min(year) + 1) %>%  # filter to critical transition year
  distinct(year, .keep_all = TRUE) %>%  # drop duplicates
  dplyr::select(year) %>%
  ungroup()

re_path <- emerge_sim %>%
  group_by(time) %>%
  summarise(mean_re = mean(RE_seas)) %>%  
  mutate(year = round(time)) %>%  # create 'year' variable
  ungroup()

one_sim <- emerge_sim %>% 
  filter(sim == 1) %>%
  left_join(re_path, by = "time") %>%
  mutate(used = ifelse(time <= re_one_year$year, "yes", "no"))

re_series <- ggplot(one_sim, aes(x = time, y = mean_re)) +
  geom_line(color = "tan", aes(alpha = used)) +
  geom_hline(aes(yintercept = 1), color = "grey35", linetype = 3) +
  geom_vline(aes(xintercept = re_one_year$year), color = "dodgerblue4", linetype = 2) +
  annotate(geom = "text", x = 10, y = 0.2, label = "critical\nyear", size = 3, color = "dodgerblue4") +
  scale_alpha_manual(values = c(0.4, 1)) +
  guides(alpha = FALSE) +
  labs(x = "Simulation time (year)", y = expression(R[E])) +
  theme_classic(base_size = 12)

cases_series <- ggplot(filter(one_sim, time < 11.5), aes(x = time, y = reports)) +
  geom_line(color = "tan", aes(alpha = used)) +
  geom_vline(aes(xintercept = 0), color = "dodgerblue4", linetype = 2) +
  geom_vline(aes(xintercept = re_one_year$year), color = "dodgerblue4", linetype = 2) +
  geom_vline(aes(xintercept = 7/2)) +
  annotate(geom = "text", x = 1.75, y = 250, label = "Null window") +
  annotate(geom = "text", x = 5.25, y = 250, label = "Test window") +
  scale_alpha_manual(values = c(0.4, 1)) +
  guides(alpha = FALSE) +
  annotate(geom = "text", x = 7.35, y = 150, label = "critical\nyear", size = 3, color = "dodgerblue4") +
  labs(x = "Simulation time (year)", y = "Reported cases") +
  theme_classic(base_size = 12)

plot_grid(re_series, cases_series, nrow = 2, align = "v", 
          scale = 0.9, labels = "AUTO", label_size = 12)
```

For simulations of disease elimination by increasing the vaccination coverage of the population, we define the critical time as the time at which vaccination coverage reaches the threshold needed for herd immunity.
This vaccination threshold is defined as $p = 1 - 1/R_0$.
Because our transmission function is seasonal, we first calculated time-specific $R_0$ as: $R_0(t) = \frac{\eta \beta_t \mu}{\nu(\eta+\nu)(\gamma+\nu)}$, where $1/\eta$ is the incubation period, $1/\gamma$ is the infectious period, $\beta_t$ is the time-specific rate of transmission, $\mu$ is the birth rate, and $\nu$ is the death rate. Only $\beta_t$ is estimated by our model. We set $1/\eta$ = 8 days, $1/\gamma$ = 5 days, and $\mu = \nu$ = 0.05.
Figure 2 in the main text shows these values.
We took a conservative approach for calculating the vaccination threshold by using the maximum value of $R_0(t)$, such that: $p = 1 - 1/\text{max}(R_0(t))$.
We set the time at which vaccination coverage is equal to *p* as the endpoint for the EWS analysis.
All elimination simulations had vaccination campaigns that started at year 50.
So, we define the test interval as the times between year 50 and the year at which vaccination coverage is equal to *p*.
We then defined the null interval as a window with length equal to the test interval and ending at year 49.
Figure \ref{elimin-sim} shows a typical example.

```{r elim-ex, fig.width=6, fig.cap="A typical elimination simulation for Maradi where the rate to full vaccination coverage is 1.5e-05. The null and test intervals are defined based on the time at which the vaccination campaign begins (year 50, black line) and the year at which vaccination coverage reaches the vaccination threshold for herd immunity (right-most dashed blue line). The beginning of the null interval is determined by the length of the series from time 50 to the time of herd immunity: the null interval ends at time 50 (when vaccination campaign begins) and starts at whatever time results in a series that is equal in length to the test interval. \\label{elimin-sim}"}

example_data <- readRDS("../simulations/single-elimination-example.RDS")[[1]]
ews_data_example <- readRDS("../simulations/single-elimination-example.RDS")[[2]]

rho_curve_ramp <- function(t, start = 50, speed = 1.5e-05){
  ifelse(
    t <= start,
    rho <- 0.7,
    # rho <- 0.3 * (1 - exp((t - start) * -0.015 speed)) + 0.7  # exponential
    rho <- min(0.7 + (t - start)*speed, 1)  # linear
  )
  return(rho)
}

years <- 100
weeks <- years*52
days <- years*365
vacc_coverage_ts <- sapply(0:days, FUN = rho_curve_ramp, 
                           start = 50*365, speed = 1.5e-05)

vacc_ex_tbl <- tibble(
  time = 1:length(vacc_coverage_ts),
  coverage = vacc_coverage_ts
)


vacc_plot <- ggplot(vacc_ex_tbl, aes(time/365, coverage)) +
  geom_line(color = "dodgerblue4") +
  geom_hline(aes(yintercept = 0.7), linetype = 2, 
             size = 0.25, color = "grey45") +
  geom_hline(aes(yintercept = unique(example_data$threshold)), linetype = 2, 
             size = 0.25, color = "grey45") +
  annotate(geom = "text", x = 81, y = 0.72, color = "grey45",
           label = "Current vaccination coverage in Niger", size = 3) +
  annotate(geom = "text", x = 25, y = 0.91, color = "grey45",
           label = "Vaccination coverage for herd immunity", size = 3) +
  scale_y_continuous(limits = c(0.7, 1)) +
  labs(x = NULL, y = "Vaccination\ncoverage") +
  theme_classic(base_size = 14)

ts_plot <- ggplot() +
  geom_line(data = example_data, aes(x = time, y = reports, group = group), color = "tan", alpha = 0.4, size = 0.3) +
  geom_vline(data = ews_data_example, aes(xintercept = max(time)), color = "dodgerblue4",  linetype = 2) +
  geom_vline(data = ews_data_example, aes(xintercept = min(time)), color = "dodgerblue4",  linetype = 2) +
  geom_vline(data = ews_data_example, aes(xintercept = 50), color = "grey45", linetype = 1) +
  geom_line(data = ews_data_example, aes(x = time, y = reports), color = "tan", size = 0.3) +
  annotate(geom = "text", x = 28, y = 1450, label = "Null window") +
  annotate(geom = "text", x = 70, y = 1450, label = "Test window") +
  annotate(geom = "text", x = 36, y = 900, label = "start of\nvaccination campaign", size = 3, color = "grey25") +
  annotate(geom = "text", x = 79, y = 900, label = "coverage reaches\nherd immunity", size = 3, color = "dodgerblue4") +
  labs(x = "Simulation time (year)", y = "Reported cases") +
  theme_classic(base_size = 14)

plot_grid(vacc_plot, ts_plot, rel_heights = c(1, 1), ncol = 1, align = "v")

```

# Early warning signals

We calculated nine early warning signals using the ```spaero::get_stats()``` function.
Formulas for the early warning signals are in Table S1.

\comment{As far as I can tell, we did not use moving windows (just two
disjoint windows), so isn't the introduction of equations with a
bandwidth (windows size) parameter unnecessarily complex? I have
substituted what seems to me a clearer alternative version of the
equations.}

\comment{April, 22, 2019: I now see that large moving windows were
used even in the two-part analysis. It seems to me that this is more
complicated than necessary and I would support using a simpler, true
two-window methods which I would expect to give similar results.}

| EWS | Estimator | Theoretical Correlation with $R_E(t)$ in an emergence scenario |
| -------------------- | ----------------------------------- | ------------------------ |
| Mean | \( \bar{y} = \sum_{i=1}^n \frac{y_i}{n} \) | Positive |
| Variance | \( s^2 = \sum_{i=1}^n \frac{(y_i - \bar{y})^2}{n} \) | Positive |
| Coefficient of variation | \( \frac{ s }{ \bar{y} } \) | Null |
| Index of dispersion | \( \frac{ s^2 }{ \bar{y} } \) | Positive |
| Skewness | \( \frac{1}{s^3} \sum_{i=1}^n \frac{\left( y_i - \bar{y} \right)^3}{n} \) | Positive |
| Kurtosis | \( \frac{1}{s^4} \sum_{i=1}^n \frac{\left( y_i - \bar{y} \right)^4}{n} \) | Positive |
| Autocovariance | \( s^2_1 =  \sum^{n}_{i=2} \frac{\left( y_i - \bar{y} \right)\left( y_{i - 1}  - \bar{y} \right)}{n} \) | Positive |
| Autocorrelation | \( r_1 = \frac{s^2_1 }{ s^2} \) | Positive |
| Decay time | \( -1 / \text{ln}(r_1) \) | Positive |

Table: List of candidate early warning signals and their estimating equations. Note that *b* denotes the bandwidth. See @Brett2018 for details.

# Trends of early warning signals

Theory suggests that most EWS should increase as disease dynamics
approach a critical transition from below (emergence).  When
approaching the critical transition from above (elimination), a
simple linear noise approximation to the SEIR model dynamics leads to
the prediction that the mean and variance should decrease while the
autocorrelation should develop a longer period.

\comment{If you would like a reference, I would suggest citing
"Stochastic dynamics on slow manifolds" by Constable et al. They
describe a technique for calculating the linear noise approximation
for the SEIR model with seasonality and plot the power spectrum.}



To confirm that the EWS are behaving as expected, and to document cases in which they do not, we plotted histograms of the EWS for the test and null intervals for both simulation types, emergence and elimination.

\comment{The titles for Decay time and Index of dispersion facets are
clipped off in some instances. I would be okay with eliminating decay
time if that helps, since it a simple transform of autocorrelation.}

```{r ews-hists, message=FALSE, warning=TRUE, echo = FALSE, fig.cap="Histograms of EWS from emergence simulations where susceptible depletion fraction was 1e-04.", fig.height=5}
emerge_ews <- read.csv("../results/ews-emergence.csv") %>%
  filter(metric != "variance_first_diff" & susc_discount == 0.0)  %>%
  mutate(metric = as.character(metric)) %>%
  mutate(
    metric = ifelse(metric == "variance", "Variance", metric),
    metric = ifelse(metric == "variance_first_diff", "Var. 1st Diff.", metric),
    metric = ifelse(metric == "autocovariance", "Autocovar.", metric),
    metric = ifelse(metric == "autocorrelation", "Autocorr.", metric),
    metric = ifelse(metric == "decay_time", "Decay time", metric),
    metric = ifelse(metric == "mean", "Mean", metric),
    metric = ifelse(metric == "index_of_dispersion", "Index of dis.", metric),
    metric = ifelse(metric == "coefficient_of_variation", "Coeff. var.", metric),
    metric = ifelse(metric == "skewness", "Skewness", metric),
    metric = ifelse(metric == "kurtosis", "Kurtosis", metric)
  )

gout <- list()
for(do_city in sort(unique(emerge_ews$city))){
  if(do_city != "Zinder"){
    gout[[do_city]] <- ggplot(filter(emerge_ews, city == do_city), aes(fill = half, x = value)) +
      geom_histogram(bins = 20) +
      facet_wrap(~metric, scales = "free", nrow = 1) +
      scale_fill_manual(values = c("tan", "dodgerblue4")) +
      theme_minimal(base_size = 8) +
      theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
      guides(fill = FALSE) +
      labs(y = "Count", x = "") +
      ggtitle(do_city)
  }

  if(do_city == "Zinder"){
    gout[[do_city]] <- ggplot(filter(emerge_ews, city == do_city), aes(fill = half, x = value)) +
      geom_histogram(bins = 20) +
      facet_wrap(~metric, scales = "free", nrow = 1) +
      scale_fill_manual(values = c("tan", "dodgerblue4"), labels = c("Null", "Test"), name = NULL) +
      theme_minimal(base_size = 8) +
      theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
      labs(y = "Count", x = "EWS value") +
      ggtitle(do_city)
  }

}

ews_hists <- cowplot::plot_grid(
  plotlist = gout,
  nrow = 4,
  labels = "AUTO"
)
print(ews_hists)
```

```{r ews-hists2, message=FALSE, warning=TRUE, echo = FALSE, fig.cap="Histograms of EWS from elimination simulations where rate of vaccination coverage reaching 1 is 1.5e-05.", fig.height=5}
elimination_ews <- read.csv("../results/ews-elimination.csv") %>%
  filter(metric != "variance_first_diff" & vacc_speed == 1.5e-05) %>%
  mutate(metric = as.character(metric)) %>%
  mutate(
    metric = ifelse(metric == "variance", "Variance", metric),
    metric = ifelse(metric == "variance_first_diff", "Var. 1st Diff.", metric),
    metric = ifelse(metric == "autocovariance", "Autocovar.", metric),
    metric = ifelse(metric == "autocorrelation", "Autocorr.", metric),
    metric = ifelse(metric == "decay_time", "Decay time", metric),
    metric = ifelse(metric == "mean", "Mean", metric),
    metric = ifelse(metric == "index_of_dispersion", "Index of dis.", metric),
    metric = ifelse(metric == "coefficient_of_variation", "Coeff. var.", metric),
    metric = ifelse(metric == "skewness", "Skewness", metric),
    metric = ifelse(metric == "kurtosis", "Kurtosis", metric)
  )

gout <- list()
for(do_city in sort(unique(emerge_ews$city))){
  if(do_city != "Zinder"){
    gout[[do_city]] <- ggplot(filter(elimination_ews, city == do_city), aes(fill = half, x = value)) +
      geom_histogram(bins = 20) +
      facet_wrap(~metric, scales = "free", nrow = 1) +
      scale_fill_manual(values = c("tan", "dodgerblue4")) +
      theme_minimal(base_size = 8) +
      theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
      guides(fill = FALSE) +
      labs(y = "Count", x = "") +
      ggtitle(do_city)
  }

  if(do_city == "Zinder"){
    gout[[do_city]] <- ggplot(filter(elimination_ews, city == do_city), aes(fill = half, x = value)) +
      geom_histogram(bins = 20) +
      facet_wrap(~metric, scales = "free", nrow = 1) +
      scale_fill_manual(values = c("tan", "dodgerblue4"), labels = c("Null", "Test"), name = NULL) +
      theme_minimal(base_size = 8) +
      theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
      labs(y = "Count", x = "EWS value") +
      ggtitle(do_city)
  }

}

ews_hists <- cowplot::plot_grid(
  plotlist = gout,
  nrow = 4,
  labels = "AUTO"
)
print(ews_hists)
```

# Bootstrapped parameter confidence intervals

We used a bootstrap approach to estimate approximate 95\% confidence intervals for all parameters.
Below we show summary statistics for all parameters except the B-splines.
For B-splines, and transmission rate more generally, we plot the seasonal transmission rate function for all bootstrap replicates (Figure \ref{season-trans}).

```{r param-summaries, message=FALSE, warning=TRUE, echo = FALSE, results = "asis"}

# Define computation grid -------------------------------------------------

nboots <- 100
nmifs <- 50
comp_grid <- expand.grid(1:nboots, 1:50)
colnames(comp_grid) <- c("boot_series", "param_set")
comp_grid$do_grid <- 1:nrow(comp_grid)


# Load loglikelihood results and combine ----------------------------------

result_files <- list.files("../results/")
mle_files <- result_files[grep("initial-mif-lls", result_files)]

mles <- tibble()
for(do_file in mle_files){
  tmp <- read.csv(paste0("../results/", do_file)) %>%
    drop_na() %>%
    filter(loglik == max(loglik)) %>%
    dplyr::select(loglik, loglik_se, beta_mu, beta_sd, iota, rho, tau, S_0, E_0, I_0) %>%
    gather(key = parameter, value = mle) %>%
    mutate(city = str_sub(do_file, 17, (nchar(do_file)-4)))
  
  mles <- bind_rows(mles, tmp)
}

loglik_files <- list.files("../results/", pattern = "bootstrap-mif-lls")
param_summaries <- tibble()

for(do_file in loglik_files){
  do_city <- str_sub(do_file, 19, -5)
  tmp_file <- read.csv(paste0("../results/", do_file)) %>%
    slice(2:n()) %>%  # ignore first row of NAs
    left_join(comp_grid, by = "do_grid") %>%  # merge in comp grid info
    group_by(boot_series) %>%
    filter(loglik == max(loglik)) %>%
    ungroup() %>%
    dplyr::select(-do_grid, -loglik, -loglik_se, -boot_series, -param_set,
                  -b1, -b2, -b3, -b4, -b5, -b6) %>%
    gather(key = parameter, value = mle_value) %>%
    group_by(parameter) %>%
    summarise(
      mean_value = mean(mle_value),
      median_value = median(mle_value),
      std_dev = sd(mle_value),
      upper_95_ci = quantile(mle_value, 0.975),
      lower_95_ci = quantile(mle_value, 0.025)
    ) %>%
    mutate(city = do_city)
  
  param_summaries <- bind_rows(param_summaries, tmp_file)
  }

mle_and_boots <- left_join(mles, param_summaries, by = c("parameter", "city"))

# Load example pomp file for basis function
pomp_file <- "../code/measles-pomp-object-Agadez.RDS"
measles_pomp <- readRDS(pomp_file)  # exemplar bases


# Calculate seasonal transmission functions -------------------------------

bases <- as_tibble(measles_pomp@covar) %>%
  dplyr::select(starts_with("x")) %>%
  dplyr::slice(1:365) %>%
  mutate(
    day = 1:365
  ) %>%
  gather(key = base, value = value, -day)

season <- bases %>%
  spread(key = base, value = value) %>%
  dplyr::select(-day) %>%
  as.matrix()

seasonal_functions <- tibble()
for(do_file in loglik_files){
  do_city <- str_sub(do_file, 19, -5)
  
  b_splines <- read.csv(paste0("../results/", do_file)) %>%
    slice(2:n()) %>%  # ignore first row of NAs
    left_join(comp_grid, by = "do_grid") %>%  # merge in comp grid info
    group_by(boot_series) %>%
    filter(loglik == max(loglik)) %>%
    ungroup() %>%
    dplyr::select(b1, b2, b3, b4, b5, b6)
  
  betas <- read.csv(paste0("../results/", do_file)) %>%
    slice(2:n()) %>%  # ignore first row of NAs
    left_join(comp_grid, by = "do_grid") %>%  # merge in comp grid info
    group_by(boot_series) %>%
    filter(loglik == max(loglik)) %>%
    ungroup() %>%
    dplyr::select(beta_mu)
  
  seasonal_betas <- tibble()
  for(i in 1:nrow(b_splines)){
    qis <- as.numeric(b_splines[i, ])
    beta_tmp <- as.numeric(betas[i, "beta_mu"])
    
    seasonal_tmp <- tibble(
      beta = as.numeric((1+exp(season %*% qis)) * beta_tmp),
      day = 1:365,
      boot = i
    )
    seasonal_betas <- bind_rows(seasonal_betas, seasonal_tmp)
  }  # end bootstrap loop
  
  seasonal_betas <- seasonal_betas %>%
    mutate(city = do_city)
  
  seasonal_functions <- bind_rows(seasonal_functions, seasonal_betas)
  
}  # end city loop

param_summaries <- mle_and_boots %>%
  mutate(
    parameter = ifelse(parameter == "loglik", "Log likelihood", parameter),
    parameter = ifelse(parameter == "loglik_se", "Log likelihood S.E.", parameter),
    parameter = ifelse(parameter == "beta_mu", "$\\beta$", parameter),
    parameter = ifelse(parameter == "beta_sd", "$\\sigma$", parameter),
    parameter = ifelse(parameter == "rho", "$\\rho$", parameter),
    parameter = ifelse(parameter == "iota", "$\\psi$", parameter),
    parameter = ifelse(parameter == "S_0", "$S_{t=0}$", parameter),
    parameter = ifelse(parameter == "E_0", "$E_{t=0}$", parameter),
    parameter = ifelse(parameter == "I_0", "$I_{t=0}$", parameter),
    parameter = ifelse(parameter == "tau", "$\\tau$", parameter)
  ) %>%
  select(parameter, mle, mean_value, median_value, std_dev, lower_95_ci, upper_95_ci, city) %>%
  rename(
    `Parameter` = parameter,
    `MLE` = mle,
    `Mean` = mean_value,
    `Median` = median_value,
    `SD` = std_dev,
    `Upper 95% CI` = upper_95_ci,
    `Lower 95% CI` = lower_95_ci
  )
```

```{r param-tables, message=FALSE, warning=TRUE, echo = FALSE, results = "asis"}

for(do_city in unique(param_summaries$city)){
  tmp_table <- param_summaries %>%
  filter(city == do_city) %>%
  select(-city) %>%
  mutate_if(is.numeric, ~as.character(signif(., 3)))

  docap <- paste0("Maximum likelihood estimates and summary statistics of parameters from bootstrapped estimates for ", do_city, ".")
  print(knitr::kable(tmp_table, format = "pandoc", caption = docap))
}
```

```{r plot-seasonal-transmission, fig.cap="Bootstrap replicates of estimated seasonal transmission functions. \\label{season-trans}", fig.width=8.5, fig.height=3}

seasonal_functions <- seasonal_functions %>%
  mutate(
    date = as.Date(day, origin = "2016-12-31", tz = "UTC")
  )

ggplot(seasonal_functions, aes(x = date, y = beta, group = boot)) +
  geom_line(alpha = 0.3, color = "dodgerblue4") +
  labs(x = "Date", y = expression(paste("Tranmission rate, ",beta, " (", yr^-1, ")"))) +
  scale_x_date(date_labels = "%b", date_breaks = "2 months") +
  facet_wrap(~city, nrow = 1) +
  theme_minimal()

```

# Length of null and test intervals

In the main text we show EWS performance across the range of susceptible discounting factors and vaccination campaign speeds we modeled.
To put these values into context, here we show the corresponding window sizes for each simulation.
These window sizes are the number of weeks that occur in the null and test intervals (Figure \ref{bws}).

```{r bw-weeks, fig.width=8.5, fig.height=4, fig.cap="Length of null and test intervals (in weeks) for emergence and elimination simulations at different parameter values.  \\label{bws}"}
read_csv("../results/emergence-bandwidths-colnames.csv",
         col_types = cols(
           row_number = col_integer(),
  city = col_character(),
  susc_discount = col_double(),
  bandwidth = col_integer()
)) %>%
  ggplot(aes(x = susc_discount, y = bandwidth, color = city)) +
  geom_line() +
  geom_point(size = 3) +
  labs(x = "Susceptible discounting factor", 
       y = "Number of weeks in interval",
       title = "Length of emergence intervals") +
  scale_color_colorblind(name = NULL) +
  guides(color = FALSE) +
  theme_minimal() -> emerge_ints

read_csv("../results/elimination-bandwidths-colnames.csv",
         col_types = cols(
  row_number = col_integer(),
  city = col_character(),
  vacc_speed = col_double(),
  bandwidth = col_integer()
)) %>%
  ggplot(aes(x = vacc_speed, y = bandwidth, color = city)) +
  geom_line() +
  geom_point(size = 3) +
  labs(x = "Speed of vaccination campaign", 
       y = "Number of weeks in interval",
       title = "Length of elimination intervals") +
  scale_color_colorblind(name = NULL) +
  theme_minimal() +
  theme(legend.position = c(0.8,0.8)) -> elimin_ints

cowplot::plot_grid(emerge_ints, elimin_ints, nrow = 1, align = "h")

```

# Moving window analysis

To complement our fixed window analysis presented in the main text, we conducted a moving window analysis as might be conducted on real surveilance data.
We used the same null and test intervals as described above for the emergence and elimination scenarios.
However, instead of calculating a single value for each EWS over the entire window of the interval, we calculated EWS in the intervals over moving windows of 35 weeks.
We then calculated the Kendall's rank correlation ($\tau$) between each EWS and time in each of the null and test intervals.
We used the distributions of Kendall's $\tau$ over the 500 replicate simulations to then calculate the Area Under the Curve (AUC) metric for each EWS.
For the approach to elimination, the results are similar to those for the fixed window analysis (Figure \ref{mvw-ews}B).
For the approach to emergence, however, the moving window results show much worse performance for Agadez and Zinder, and, on average, the AUC values are lower for all EWS in each city (Figure \ref{mvw-ews}A).

\comment{I see now that the original version of Table S3 with moving
window statistics does have value for interpretation of this
section. Perhaps it would make sense to include both versions? If
there is only one verion, I would still say the simpler one is
preferable to me. We can cite Brett et al. "Anticipating disease
emergence" for the more complicated version. }

```{r plot-aucs, fig.width=8.5, fig.height=5.5, fig.cap="Performance of early warning signals (EWS) over a moving window. Correlations between EWS (calculated over 35 week moving windows) and time were calculated over two intervals, one far from a critical transition and one near, for simulations of re-emergence (A) and elimination (B). EWS performance is quantified using the AUC metric, which we show here as a heatmap of AUC values. AUC values farther from 0.5 indicate higher ability to distinguish among time series near and far from a critical transition.  \\label{mvw-ews}"}
library(viridis)
emergence_aucs <- read.csv("../results/emergence-mvw-grid-aucs.csv")
elimination_aucs <- read.csv("../results/elimination-mvw-grid-aucs.csv")

emerge_plot <- ggplot() +
  geom_tile(data = emergence_aucs, aes(x = as.factor(susc_discount), y = metric, fill = AUC)) +
  scale_fill_viridis(limits = c(0,1), direction = -1, option = "E", name = "AUC") +
  facet_wrap(~city, nrow = 1) +
  labs(x = "Level of susceptible depletion", y = NULL) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  theme(panel.spacing = unit(1, "lines"),
        plot.title = element_text(size = 11, face = "bold")) +
  ggtitle("Anticipating emergence over a moving window")

eliminate_plot <- ggplot() +
  geom_tile(data = elimination_aucs, aes(x = as.factor(vacc_speed*10000), y = metric, fill = AUC)) +
  scale_fill_viridis(limits = c(0,1), direction = -1, option = "E", name = "AUC") +
  facet_wrap(~city, nrow = 1) +
  labs(x = expression(paste("Rate to full vaccine coverage (", phantom()%*%phantom(), 10^4, ")")), y = NULL) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  theme(panel.spacing = unit(1, "lines"),
        plot.title = element_text(size = 11, face = "bold")) +
  ggtitle("Anticipating elimination over a moving window")

cowplot::plot_grid(emerge_plot, eliminate_plot, nrow = 2, align = "v", labels = "AUTO", label_size = 12)
```


# Susceptible depletion after outbreaks

We *a priori* defined the magnitude of susceptible depletion in our models of measles re-emergence after an outbreak.
Thus, a potential limit to our study is that the actual magnitudes of susceptible depletion are less than the values we modeled.
That is, maybe we modeled very unrealistic levels of susceptible depletion, making our results interesting but not useful.
To test the importance of this limitation, we calculated the relative decline of the susceptible class after outbreak years.
We used the 100 replicate simulations that were used for parametric bootstrapping.
These simulations adequately reproduce the general dynamics of measles in each city, including outbreak size.
We used the simulations because the replicates gave us more outbreaks to work with than the observed data alone.

For each of the 100 replicates for each city, we defined an "outbreak year" as any year in which the cumulative number of reported cases exceeded 80% of the maximum number of cumulative cases observed across all years.
We then calculated susceptible depletion (i.e., the susceptible discounting fraction used in our modeling study) as the minimum number of susceptible individuals in the outbreak year divided by the maximum number of susceptible individuals in the previous year.
For example, let year *t* be an outbreak year where the cumulative number of cases exceeded 80% of the maximum cumulative cases observed.
The level of susceptible depletion between year $t-1$ and year $t$ is then: $\text{min}(S_t)/\text{max}(S_{t-1})$.
Essentially, this metric tells us the discounting fraction that would need to be applied to *S* to simulate an outbreak of the size in year *t*.
Histograms for each city are in Figure \ref{susc-disc}.

```{r, fig.width=4, fig.height=3, fig.cap="Histograms of susceptible depletion discounting factors for each city. Values were calculated by comparing the number of susceptible individuals before and after large outbreaks from 100 replicate simulations from the fitted models for each city.  \\label{susc-disc}"}
# Define threshold for outbreak year --------------------------------------

# Outbreaks defined as: an annual case count exceeding x% of the maximum
# annual case count for a region, where x is the threshold set below.
outbreak_threshold <- 0.8  # 80% of max = outbreak year


# Load simulated data -----------------------------------------------------

cities <- c("Agadez", "Maradi", "Niamey", "Zinder")
sim_data <- tibble()
for(do_city in cities){
  tmp_file <- paste0("../simulations/bootstrap-sims-", do_city, ".RDS")
  tmp_data <- readRDS(tmp_file) %>%
    unnest(cols = data) %>%
    mutate(city = do_city)
  sim_data <- bind_rows(sim_data, tmp_data)
}


# Identify outbreak years and calculate S depletion -----------------------

outbreak_years <- sim_data %>%
  mutate(year = trunc(time)) %>%
  group_by(city, sim, year) %>%
  summarise(total_cases = sum(reports),
            max_S = max(S),
            min_S = min(S)) %>%
  mutate(outbreak = ifelse(total_cases > outbreak_threshold*max(total_cases), 
                           TRUE, FALSE),
         max_S_t_minus_1 = lag(max_S),
         susc_depletion = min_S / max_S_t_minus_1) %>%
  filter(outbreak == TRUE) %>%
  drop_na()


# Calculate fraction of depletions less than 0.5 --------------------------

fracs_less_than_half <- outbreak_years %>%
  ungroup() %>%
  group_by(city) %>%
  mutate(
    deplete_half = ifelse(susc_depletion < 0.5, TRUE, FALSE)
  ) %>%
  group_by(city, deplete_half) %>%
  count() %>%
  group_by(city) %>%
  mutate(
    frac_less = n/sum(n)
  ) %>%
  filter(deplete_half == TRUE) %>%
  dplyr::select(city, frac_less)


# Plot boxplots of susceptible depletion ----------------------------------

ggplot() +
  geom_histogram(data = filter(outbreak_years, city == "Agadez"), 
                 aes(x = susc_depletion, fill = city), 
                 color = "white", binwidth = 0.05) +
  geom_histogram(data = filter(outbreak_years, city == "Maradi"), 
                 aes(x = susc_depletion, fill = city), 
                 color = "white", binwidth = 0.05) +
  geom_histogram(data = filter(outbreak_years, city == "Niamey"), 
                 aes(x = susc_depletion, fill = city), 
                 color = "white", binwidth = 0.05) +
  geom_histogram(data = filter(outbreak_years, city == "Zinder"), 
                 aes(x = susc_depletion, fill = city), 
                 color = "white", binwidth = 0.05) +
  geom_vline(aes(xintercept = 0.5), color = "grey25", linetype = 2) +
  labs(y = "Number of outbreaks", x = "Level of susceptible depletion") +
  scale_fill_colorblind(name = NULL) +
  theme_minimal(base_size = 12)
```



# References
