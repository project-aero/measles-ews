---
title: "Compare EWS and model-based outbreak detection"
author: "Andrew Tredennick"
date: 2019-02-07
output: html_notebook
---

```{r setup, message=FALSE, warning=FALSE}
library(tidyverse)
library(lubridate)
library(pomp)
library(foreach)
library(doParallel)

knitr::opts_chunk$set(message = FALSE, warning = FALSE)
```

# Motivation
Parametric models of disease dynamics are data hungry and take a lot of time and effort to fit.
However, their mechanistic basis may lead to better predictions of outbreaks in highly seasonal systems.
Model-independent early warning signals (EWS), on the other hand, lack mechanism but are easy to calculate and require less data.
An open question is whether parametric models are worth the effort.
That is, are EWS just as good for half the hassle?

In this notebook, I answer the above question by predicting large vs. small outbreaks from a fitted parametric *SEIR* model and from EWS.

# Methods

## Defining large and small outbreaks
The first step is to classify the seasonal epidemics as outbreaks and non-outbreaks.
To do this, I will fit a general mixture model of two exponentials.
The break point between the two defines the incidence at which we classify an outbreak as large or small.
I use incidence, rather than raw case numbers, so I can combine the data from all the cities, increasing the sample size to a decent level.

```{r classify-outbreaks}
# Load measles data
fname <- "../../data/clean-data/weekly-measles-incidence-niger-cities-clean.RDS"
measles_data <- readRDS(fname) %>%
  filter(year > 1994)  # remove 1994 year, only used for modeling

# Make population df
pop_data <- tibble(
  region = c("Agadez (City)", "Maradi (City)", 
             "Niamey (City)", "Zinder (City)"),
  pop = c(118224, 267249, 1027000, 322935)  # from Wikipedia
)

# Summarize to cumulative yearly cases
cusum_data <- measles_data %>%
  group_by(region, year) %>%
  summarise(cases = sum(cases)) %>%
  left_join(pop_data, by = "region") %>%  # add in population data
  mutate(incidence = cases/pop)  # calculate incidence

x <- sort(cusum_data$incidence)  # extract incidence data

# Fit null distribution for starting values
fit1 <- MASS::fitdistr(x, "exponential")

# Define likelihood of mixed exponential
mixexplik <- function(p, lambda1, lambda2) {
  z <- p*dexp(x,lambda1) + (1-p)*dexp(x,lambda2)
  return(-sum(log(z)))
}

# Fit the model
mle_fit <- suppressWarnings(  # ignore warnings about bad guesses
  stats4::mle(minuslogl = mixexplik, 
              start= list(p = 0.2,
                          lambda1 = as.numeric(fit1$estimate), 
                          lambda2 = as.numeric(fit1$estimate)), 
              method="Nelder-Mead")
)

# Calculate the boundary incidence between small and large outbreaks
outbreak_boundary <- qexp(p = 0.5, 
                          rate = mle_fit@coef["p"]*mle_fit@coef["lambda1"])
```

The GMM suggests that the boundary between small and large outbreaks is an incidence level of ```r round(outbreak_boundary, 4)```.

## Simulate a new series
The next step is to simulate a new time series from the fitted models.
I do this because we want to predict outbreaks as if we are dealing with new data, not the same exact data used to fit the model.
For now, I am just going to focus on Niamey as a test case.

```{r sim-new-data}
# Load pomp object
pomp_object <- readRDS("../../code/measles-pomp-object-Niamey.RDS")

# Load MLEs
mle_file <- paste0("../../results/initial-mif-lls-Niamey.csv")
mles <- read.csv(mle_file) %>%
  as_tibble() %>%
  filter(loglik == max(loglik, na.rm = TRUE)) %>%
  dplyr::select(-do_grid, -loglik, -loglik_se)

sim <- as_tibble(
    simulate(pomp_object, params = mles, nsim = 1, as.data.frame = TRUE,
             include.data = FALSE, seed = 12345278)
  ) %>%
  filter(time >= 1995) %>%
  dplyr::select(sim, time, reports, S, E, I, RE_seas) %>%
  mutate(year = trunc(time)) %>%
  group_by(year) %>%
  mutate(cumulative_cases = sum(reports)) %>%
  mutate(incidence = cumulative_cases/1027000) %>%  # divide cases by population
  mutate(outbreak_year = ifelse(incidence < outbreak_boundary, FALSE, TRUE)) %>%
  ungroup() %>%
  mutate(date = unique(measles_data$date))

ggplot(sim, aes(x = date, y = reports)) +
  geom_line(aes(group = year, color = outbreak_year)) +
  ggtitle("Simulated time series from the MLE model")
```

## Predict trajectories of the new series
Now we can use the fitted model to predict trajectories, informed by data, via particle filtering.
The idea is to use particle filtering up to a certain time point *t* and then make forecasts for $t + 1, t + 2, t + 3, \dots, t + T$, where *T* is the final week we wish to forecast.
To start, *t* will be the end of the year and *T* will be the end of the next year.
Eventually we will look across forecasts that start from $t-0, t-1, t-2, t-3, \dots, t-k$, where *k* is a week very close to the end of the previous year's measles season (around calendar week 40).

As an initial attempt, I will focus on the large outbreak in 2000 of the simulated series.
The "data" will be reports from week 40 to week 52 in 1999.
Forecasts will be made for all of 2000 (52 weeks).

```{r plot-data-used}
test_data <- sim %>%
  filter(year(date) < 2000) %>%
  filter(year(date) == 1999 | (year(date) == 1998 & week(date) > 30)) %>%
  mutate(split = ifelse(year(date) == 1998, 
                        "initial conditions", 
                        "to be forecast"))

ggplot(test_data, aes(x = date, y = reports)) +
  geom_line(aes(color = split)) +
  ggtitle("Data for initial conditions and to be forecast")
```

See this, https://kingaa.github.io/sbied/ebola/ebola.html, for ideas on setting up a particle filter that transitions to forecasting.

## Example forecast

Here's an example of the plan.
The plan is to have a given set of initial conditions, from particle filtering up to a certain point, and then simulating replicate trajectories of the next year starting from those initial conditions.
I can then tally the number of replicates that are predicted to be outbreaks to compute the probability of an outbreak.
The initial conditions can be updated to look at difference lead times.
Below is just an example of how the replicate simulations allow me to compute the probability of an outbreak, conditional on the initial conditions and the model parameters.

```{r ex-forecast}
simulate(pomp_object, params = unlist(mles), nsim = 100, 
         as.data.frame = TRUE, include.data = FALSE) %>%
  filter(time >= 1999 & time < 2000) %>%
  dplyr::select(time, reports, sim) %>%
  group_by(sim) %>%
  mutate(population = as.numeric(pop_data[3, 2]),
         total_cases = sum(reports),
         incidence = total_cases/population,
         outbreak_year = ifelse(incidence >= outbreak_boundary, 
                                TRUE, FALSE)) %>%
  ungroup() %>%
  group_by(outbreak_year) %>%
  mutate(prop_outbreak = n()/52/100) %>%
  ungroup() -> example_simulation

pr_outbreak <- example_simulation %>%
  filter(outbreak_year == TRUE) %>%
  pull(prop_outbreak) %>%
  unique()

  ggplot(example_simulation, aes(x = time, y = reports, group = sim)) +
    geom_line(aes(color = outbreak_year), size = 0.4) +
    ggtitle("Example simulations of a given year", 
            subtitle = paste0("Pr(outbreak) = ", pr_outbreak))
  
```

## Forecasting functions

Here I define a set of functions that I will use to estimate states (initial conditions) via particle filtering and simulate forecasts of the next year conditional on those initial conditions.
Note that these functions do not currently allow for parameter uncertainty, but initial conditions uncertainty is included by simulating forecasts from the filtered states, for which we have one estimate per particle.

```{r forecast-functions, eval = TRUE}

filter_states <- function(pomp_obj, param_df, num_particles = 2000){
  pf <- pomp::pfilter(pomp_obj, params = unlist(param_df), 
                      Np = num_particles, save.states = TRUE)
  pf@saved.states[-1]
}

simulate_over_obs <- function(pomp_obj, param_df, nreps){
  pp <- pomp::parmat(unlist(param_df), nrep = nreps)
  pomp::simulate(pomp_obj, params = pp, include.data = TRUE, 
                 as.data.frame = TRUE)
}

simulate_forecasts <- function(pomp_obj, train_sims, time_start, time_end,
                               param_df, nreps, states_list){ 
  start_time_id <- which(round(time(pomp_obj),3) == time_start)
  end_time_id <- which(round(time(pomp_obj),3) == time_end)
  forecast_times <- time(pomp_obj)[start_time_id:end_time_id]
  
  pomp_forecast <- pomp_obj
  time(pomp_forecast) <- forecast_times
  timezero(pomp_forecast) <- forecast_times[1]
  
  population_size_t0 <- train_sims %>% 
    filter(round(time,3) == time_start, sim == "data") %>% 
    pull(N)
  initial_proportions <- t(all_states[[start_time_id]]) %>%
    as_tibble() %>%
    dplyr::select(S, E, I) %>%
    dplyr::mutate(S_0 = S/population_size_t0,
                  E_0 = E/population_size_t0,
                  I_0 = I/population_size_t0) %>%
    dplyr::select(-S, -E, -I) %>%
    t()
    
  pp <- pomp::parmat(unlist(param_df), nrep = nreps)
  pp[rownames(initial_proportions), ] <- initial_proportions
  pomp::simulate(pomp_forecast, params = pp, include.data = FALSE, 
                 as.data.frame = TRUE)
}
```

## Run the forecasting analysis

Using this functions defined above, I can now loop over cities and make forecasts from a grid of lead times for every simulated year.
I will then plot the Pr(outbreak) as a function of lead time, with each year classified as a true outbreak year or not.

```{r run-forecasts, eval = FALSE}
# Define population data for calculating incidence
pop_data <- tibble(
  region = c("Agadez", "Maradi", "Niamey", "Zinder"),
  pop = c(118224, 267249, 1027000, 322935)  # from Wikipedia
)

# Loop over cities
all_cities <- c("Agadez", "Maradi", "Niamey", "Zinder")
do_city <- "Maradi"  # for testing

out_probs <- tibble()  # empty tibble for storage
for(do_city in all_cities){
  # Set file names
  pomp_mod_file <- paste0("../../code/measles-pomp-object-", do_city, ".RDS")
  mle_file <- paste0("../../results/initial-mif-lls-", do_city, ".csv")
  data_file <- paste0("../../data/clean-data/", 
                      "weekly-measles-incidence-niger-cities-clean.RDS")
  
  # Extract population data
  city_pop <- pop_data %>% 
    filter(region == do_city) %>% 
    pull(pop)
  
  # Load pomp object
  M1 <- readRDS(pomp_mod_file) 
  
  # Load MLE parameters
  mles <- read_csv(mle_file) %>%
    drop_na() %>%
    filter(loglik == max(loglik)) %>%
    dplyr::select(-do_grid, -loglik, -loglik_se)
  
  # Load the observed data
  the_data <- readRDS(data_file) %>%
    filter(region == paste0(do_city, " (City)"))
  
  # Map calendar dates to pomp::time
  time_date_map <- tibble(
    pomp_time = round(time(M1), 3),
    calendar_date = the_data$date,
    year = year(calendar_date),
    week = week(calendar_date)
  )
  
  # Define grid of forecasting start and stop times
  # Start times reflect different lead times, stop times are the same for all
  forecast_start_times <- time_date_map %>%
    filter(year > 1994, week > 29, year < 2005) %>%
    dplyr::select(-calendar_date, -week) 
  
  forecast_end_times <- time_date_map %>%
    filter(year > 1995, week == 52) %>%
    dplyr::select(-calendar_date, -week) %>%
    mutate(year = year - 1)
  
  # Simulate a new data series from the fitted model.
  # This series will be used for training and forecasting so that
  # we are one step away from the data used to fit the model.
  new_data <- simulate(M1, nsim = 1, params = unlist(mles), seed = 1243765,
                       as.data.frame = TRUE, include.data = FALSE) 
  
  # Update pomp object with new "data"
  M1@data["reports", ] <- t(new_data$reports)
  
  # Estimate states across particles for initial conditions at all times
  all_states <- filter_states(M1, mles, num_particles = 50)

  # Loop over all years to make forecasts from grid of lead times
  all_probs <- tibble()  # empty tibble for storage of outbreak probabilities
  for(do_year in unique(forecast_start_times$year)){
    year_time_grid <- expand.grid(
      filter(forecast_start_times, year == do_year) %>% pull(pomp_time),
      filter(forecast_end_times, year == do_year) %>% pull(pomp_time)
    )
  
    tmp_probs <- foreach::foreach(i = 1:nrow(year_time_grid), 
                                  .combine = rbind) %do%
      {
      tstart <- year_time_grid[i, 1]
      tend <- year_time_grid[i, 2]
      
      train_sims <- simulate_over_obs(M1, mles, nreps = ncol(all_states[[1]]))
      forecasts <- simulate_forecasts(pomp_obj = M1, train_sims = train_sims, 
                                      time_start = tstart, time_end = tend, 
                                      param_df = mles, 
                                      nreps = ncol(all_states[[1]]), 
                                      states_list = all_states)
      
      forecasts %>%
        mutate(year = trunc(time)) %>%
        group_by(sim, year) %>%
        summarise(total_reports = sum(reports)) %>%
        filter(year == max(year)) %>%
        ungroup() %>%
        mutate(incidence = total_reports/city_pop,
               outbreak_year = ifelse(incidence >= outbreak_boundary, 
                                      TRUE, FALSE)) %>%
        group_by(year, outbreak_year) %>%
        count() %>%
        ungroup() %>%
        mutate(week = i+29)
      }
    
    all_probs <- bind_rows(all_probs, tmp_probs)
    
  }  # end year loop


  # Add in TRUE/FALSE for each year as needed
  template <- expand.grid(c(TRUE, FALSE), 
                          unique(all_probs$year), 
                          unique(all_probs$week))
  colnames(template) <- c("outbreak_year", "year", "week") 

  new_data_classified <- new_data %>%
    dplyr::select(time, reports) %>%
    mutate(year = trunc(time)) %>%
    group_by(year) %>%
    summarise(total_cases = sum(reports)) %>%
    mutate(incidence = total_cases / city_pop,
           true_outbreak_year = ifelse(incidence >= outbreak_boundary, 
                                       TRUE, FALSE))

  save_probs <- all_probs %>%
    right_join(
      template
    ) %>%
    arrange(year, week) %>%
    mutate(n = ifelse(is.na(n) == TRUE, 0, n),
           lead_time = 52 - (week-1)) %>%
    left_join(new_data_classified) %>%
    mutate(city = do_city)
  
  out_probs <- bind_rows(out_probs, save_probs)
}

write_csv(out_probs, "parametric-outbreak-probabilities.csv")
```


## Use EWS to predict outbreaks

Now I will calculate EWS over a moving window in an increasinly large time series from week 30 to week 52 leading up to the next season.
For each length of time series (i.e., lead time) I will calculate the trend in the EWS over time.
Increasing EWS signal an upcoming transition.
When the mean, autocorrelation, autocovariance, variance, and decay time all show increasing trends, I will call that a warning of an outbreak.

```{r calc-ews}
# Get the case data
ews_data <- test_data %>%
  filter(year(date) < 2000) %>%
  dplyr::select(date, reports)

# Set the EWS bandwidth to 15 weeks
bandwidth <- 15

# Set up lead time sequence
stop_times <- seq_len(nrow(ews_data))[-c(1:3)]  # drop 1st element b/c it's 1 yr only

# Loop over stop times and calculate EWS trends
out_trends <- tibble()  # empty tibble for storage
for(tstop in stop_times){
  x <- ews_data$reports[1:tstop]
  trends <- spaero::get_stats(x = x, center_trend = "local_constant",
                              center_kernel = "uniform",
                              center_bandwidth = bandwidth, 
                              stat_trend = "local_constant",
                              stat_kernel = "uniform", 
                              stat_bandwidth = bandwidth, lag = 1,
                              backward_only = TRUE)$taus %>%
    as_tibble() %>%
    gather() %>%
    mutate(trend_sign = sign(value)) %>%
    filter(!(key %in% c("skewness", "kurtosis", 
                        "coefficient_of_variance"))) %>%
    group_by(trend_sign) %>%
    count() %>%
    mutate(stop_week = tstop)
  
  out_trends <- bind_rows(out_trends, trends)
}

out_trends %>%
  filter(trend_sign == 1) %>%
  mutate(lead_time = 52 - (29 + stop_week)) %>%
  filter(lead_time > 0 & lead_time < 21) %>%
  ggplot(aes(x = lead_time, y = n)) +
  geom_line(size = 0.4, color = "grey45") +
  geom_point(size = 4, color = "grey45") +
  scale_x_reverse() +
  labs(x = "lead time (weeks)", y = "number of EWS increasing")
```

OK, that was for on outbreak year.
Now I'll do the same analysis for every year, whether a large or small outbreak.

```{r ews-all-years}
my_get_stats <- function(x, bandwidth){
   spaero::get_stats(x = x, center_trend = "local_constant",
                     center_kernel = "uniform", center_bandwidth = bandwidth, 
                     stat_trend = "local_constant", stat_kernel = "uniform", 
                     stat_bandwidth = bandwidth, lag = 1, backward_only = TRUE)
}

get_trends <- function(x, bandwidth, ews_to_ignore = NULL){
  stop_times <- seq_len(length(x))[-c(1:3)]
  out_trends <- tibble()
  
  for(tstop in stop_times){
    xnow <- x[1:tstop]
    trends <- my_get_stats(x = xnow, bandwidth = bandwidth)$taus %>%
      as_tibble() %>%
      gather() %>%
      mutate(trend_sign = sign(value)) %>%
      filter(!(key %in% ews_to_ignore)) %>%
      group_by(trend_sign) %>%
      count() %>%
      mutate(stop_week = tstop,
             lead_time = 52 - (29 + stop_week - 1))
    
    out_trends <- bind_rows(out_trends, trends)
  }
  return(out_trends)
}

get_testing_data <- function(df, test_year){
  df %>%
    filter(year == (test_year - 1) & week(date) > 29) %>%
    pull(reports)
}

run_ews_analysis <- function(df, test_year, bw = 15, ews_to_ignore){
  cases <- get_testing_data(df, test_year)
  ews_trends <- get_trends(x = cases, bandwidth = bw,
                           ews_to_ignore = ews_to_ignore)
  return(ews_trends)
} 

out_ews_trends <- tibble()  # empty tibble for storage
for(do_city in all_cities){
  # Set file names
  pomp_mod_file <- paste0("../../code/measles-pomp-object-", do_city, ".RDS")
  mle_file <- paste0("../../results/initial-mif-lls-", do_city, ".csv")
  data_file <- paste0("../../data/clean-data/", 
                      "weekly-measles-incidence-niger-cities-clean.RDS")
  
  # Extract population data
  city_pop <- pop_data %>% 
    filter(region == paste0(do_city, " (City)")) %>% 
    pull(pop)
  
  # Load pomp object
  M1 <- readRDS(pomp_mod_file) 
  
  # Load MLE parameters
  mles <- read_csv(mle_file) %>%
    drop_na() %>%
    filter(loglik == max(loglik)) %>%
    dplyr::select(-do_grid, -loglik, -loglik_se)
  
  # Load the observed data
  the_data <- readRDS(data_file) %>%
    filter(region == paste0(do_city, " (City)"))
  
  # Map calendar dates to pomp::time
  time_date_map <- tibble(
    pomp_time = round(time(M1), 3),
    calendar_date = the_data$date,
    year = year(calendar_date),
    week = week(calendar_date)
  )
  
  # Simulate a new data series from the fitted model.
  # This series will be used for training and forecasting so that
  # we are one step away from the data used to fit the model.
  new_data <- simulate(M1, nsim = 1, params = unlist(mles), seed = 1243765,
                       as.data.frame = TRUE, include.data = FALSE)
  
  new_data_for_ews <- new_data %>%
    mutate(year = trunc(time),
           date = the_data$date) %>%
    filter(time >= 1995)
  years_to_predict <- unique(new_data_for_ews$year)[-1]
  all_trends <- tibble()
  ignore_these <-  c("skewness", "kurtosis", "cofficient_of_variation")
  
  all_trends <- tibble()  # empty tibble for storage
  for(do_year in years_to_predict){
    all_trends %>%
      bind_rows(
        run_ews_analysis(df = new_data_for_ews, test_year = do_year, 
                         bw = 15, ews_to_ignore = ignore_these) %>%
          mutate(test_year = do_year)
      ) -> all_trends
  }  # end year loop
  
  # Classify simulated years as outbreak/no outbreak
  new_data_classified <- new_data %>%
    dplyr::select(time, reports) %>%
    mutate(year = trunc(time)) %>%
    group_by(year) %>%
    mutate(total_cases = sum(reports)) %>%
    mutate(incidence = total_cases / city_pop,
           true_outbreak_year = ifelse(incidence >= outbreak_boundary, 
                                       TRUE, FALSE))
  
  all_trends <- all_trends %>% 
    mutate(city = do_city) %>%
     left_join(
    new_data_classified %>% 
      dplyr::select(year, true_outbreak_year),
    by = c("test_year" = "year")
  )
  
  out_ews_trends <- bind_rows(out_ews_trends, all_trends)
}  # end city loop



# all_trends %>%
#   filter(trend_sign == 1) %>%
#   left_join(
#     new_data_classified %>% 
#       dplyr::select(year, true_outbreak_year),
#     by = c("test_year" = "year")
#   ) %>%
#   ggplot(aes(x = lead_time, y = n, color = true_outbreak_year)) +
#   # geom_hline(aes(yintercept = 1), color = "grey45") +
#   geom_line() +
#   geom_point() +
#   scale_x_reverse() +
#   # scale_y_continuous(limits = c(0, 1)) +
#   facet_wrap(~test_year) +
#   labs(x = "lead time (weeks)", y = "proprtion of EWS increasing")
```

## Try to plot both predictions on same plot
```{r plot-both, fig.width=12, fig.height=4}
ews_trends <- out_ews_trends %>%
  filter(trend_sign == 1)

model_trends <- read_csv("parametric-outbreak-probabilities.csv") %>%
  filter(outbreak_year == TRUE)

combined_trends <- ews_trends %>%
  mutate(proportion = n/max(n),
         model = "ews") %>%
  rename("year" = test_year) %>%
  dplyr::select(model, year, proportion, lead_time, 
                true_outbreak_year, city) %>%
  bind_rows(model_trends %>%
              mutate(proportion = n/max(n),
                     model = "parametric") %>%
              dplyr::select(model, year, proportion, 
                            lead_time, true_outbreak_year, city))

ggplot(combined_trends, aes(x = lead_time, y = proportion)) +
  geom_hline(aes(yintercept = 0.5), color = "grey") +
  geom_line(aes(color = model, linetype = true_outbreak_year)) +
  facet_grid(city~year) +
  scale_x_reverse() +
  scale_y_continuous(limits = c(0,1)) +
  scale_linetype_manual(name = "true classification", 
                          labels = c("no outbreak", "outbreak"),
                          values = c(2,1)) +
  labs(x = "lead time (weeks)",
       y = paste0("proportion of ews increasing", 
                  "\nor\nproportion of simulations predicting outbreak")) +
  theme_gray(base_size = 8)
```

## Attempt a summary plot at far and near lead times

```{r summary-plot}
slice_trends <- combined_trends %>%
  filter(lead_time == 15 | lead_time == 2) 

ggplot(slice_trends, aes(x = as.factor(lead_time), y = proportion)) +
  geom_col(aes(fill = model), position = position_dodge()) +
  geom_hline(aes(yintercept = 0.5), color = "grey35") +
  facet_grid(city~true_outbreak_year) +
  labs(x = "lead time (weeks)",
       y = paste0("proportion of ews increasing", 
                  "\nor\nproportion of simulations predicting outbreak"))

true_outbreak_results <- combined_trends %>%
  filter(true_outbreak_year == TRUE) %>%
  filter(lead_time <= 20)

false_outbreak_results <- combined_trends %>%
  filter(true_outbreak_year == FALSE) %>%
  filter(lead_time <= 20)

ggplot(true_outbreak_results, aes(x = lead_time, y = as.factor(year))) +
  geom_tile(aes(fill = proportion)) +
  facet_grid(city~model) +
  scale_x_reverse() +
  scale_fill_viridis_c(option = "E") +
  ggtitle("Detection of outbreak years") +
  labs(x = "lead time (weeks)", y = "detection year") +
  theme(legend.position = "top") -> true_plot

ggplot(false_outbreak_results, aes(x = lead_time, y = as.factor(year))) +
  geom_tile(aes(fill = 1-proportion)) +
  facet_grid(city~model) +
  scale_x_reverse() +
  scale_fill_viridis_c(option = "E", name = "proportion") +
  ggtitle("Detection of non-outbreak years") +
  labs(x = "lead time (weeks)", y = "detection year") +
  theme(legend.position = "top") -> false_plot

cowplot::plot_grid(true_plot, false_plot)
```

## Plots of the simulated trajectories

```{r sim-plots}
all_new_sims <- tibble()  # empty df for storage
for(do_city in all_cities){
  # Set file names
  pomp_mod_file <- paste0("../../code/measles-pomp-object-", do_city, ".RDS")
  mle_file <- paste0("../../results/initial-mif-lls-", do_city, ".csv")
  data_file <- paste0("../../data/clean-data/", 
                      "weekly-measles-incidence-niger-cities-clean.RDS")
  
  # Extract population data
  city_pop <- pop_data %>% 
    filter(region == paste0(do_city, " (City)")) %>% 
    pull(pop)
  
  # Load pomp object
  M1 <- readRDS(pomp_mod_file) 
  
  # Load MLE parameters
  mles <- read_csv(mle_file) %>%
    drop_na() %>%
    filter(loglik == max(loglik)) %>%
    dplyr::select(-do_grid, -loglik, -loglik_se)

  
  # Simulate a new data series from the fitted model.
  # This series will be used for training and forecasting so that
  # we are one step away from the data used to fit the model.
  new_data <- simulate(M1, nsim = 1, params = unlist(mles), seed = 1243765,
                       as.data.frame = TRUE, include.data = FALSE) %>%
    mutate(year = trunc(time)) %>%
    group_by(year) %>%
    mutate(city = do_city,
           total_cases = sum(reports),
           incidence = total_cases/city_pop,
           outbreak_year = ifelse(incidence >= outbreak_boundary, 
                                  TRUE, FALSE)) %>%
    dplyr::select(city, time, reports, outbreak_year)
  
  all_new_sims <- bind_rows(all_new_sims, new_data)
}

ggplot(all_new_sims, aes(x = time, y = reports, 
                         group = trunc(time), color = outbreak_year)) +
  geom_line() +
  facet_wrap(~city, scales = "free_y")

```
