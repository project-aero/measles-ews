---
title: Early warning signals of exceptional outbreaks of measles from
  multi-scale data streams
author: "Andrew Tredennick"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

## Preliminaries
Load necessary packages. These will have to be installed via `install.packages()` if not already available on your machine.

```{r load-packages}
library(tidyverse)  # data wrangling
library(lubridate)  # time and date functions
library(ggthemes)   # pleasing ggplot2 themes
library(viridis)    # pleasing color palette
library(spaero)     # project AERO code; EWS metrics, etc.
library(forecast)   # time series functions

theme_set(theme_minimal())  # set the ggplot2 theme globally
```

And define a couple handy wrapper functions around the `spaero::get_stats()` function.

```{r helper-functions}
# Functions written by Toby Brett

get_tau <- function(stat_ts, time){
  # Calculates Kendall's tau for association of EWS metrics through time
  #
  # Args:
  #  stat_ts: a vector of EWS stats for each time window
  #  time: a sequence of time windows
  #
  # Returns:
  #  A scalar of Kendall's tau for the assocition of the metric with time
  
  stats::cor(stat_ts, time, use="pairwise.complete.obs", method="kendall")
}

get_ews_cor <- function(reports, bw = bandwidth, l = lag){
  # Calculates EWS from case reports time series and their association
  #  with time (Kendall's tau)
  #
  # Args:
  #  reports: a vector of case reports through time
  #  bw: bandwidth of kernel for detrending
  #  l: lag for autocovariance calculation
  #
  # Returns:
  #  A list of size vectors of EWS stats and a vector of Kendall's tau over 
  #  time for each EWS stat.
  
  stats <- spaero::get_stats(
    reports,
    center_bandwidth = bw, 
    stat_bandwidth = bw,
    center_trend = "local_constant",
    stat_trend = "local_constant",
    center_kernel = "uniform",
    stat_kernel = "uniform", 
    lag = l
  )
  
  taus <- sapply(stats$stats, get_tau, time = seq_along(reports))
  list(stats = stats$stats, taus = taus)
}
```

## Data
The data are weekly reports from 1995 to 2005 of measles cases from 40 deparments in Niger, which I sum over here to plot the national trend through time.

```{r load-data}
file_name <- "../niger_measles/niger_regional_1995_2005.csv"
niger_measles_raw <- read_csv(file_name, col_types = cols())

num_regions <- nrow(niger_measles_raw)
num_weeks <- ncol(niger_measles_raw) - 1  # subtract 1 from ncol() because first column are regions

# Create a vector of dates for each week over 11 years
start_date <- ymd("19950101")
all_dates <- seq(start_date, by = "week", length.out = num_weeks)

# Clean up the data frame
niger_measles <- niger_measles_raw %>%
  gather(key = week, value = cases, -X1) %>%
  mutate(
    week_num = rep(1:num_weeks, each = num_regions),
    date = rep(all_dates, each = num_regions)
  ) %>%
  dplyr::rename(region = X1) %>%
  dplyr::select(-week)

# Aggregate over departments (regions) for plotting national trend 
national_data <- niger_measles %>%
  group_by(date) %>%
  summarise(total_cases = sum(cases, na.rm = TRUE))  # ignore NAs from some regions

ggplot(national_data, aes(x = date, y = total_cases)) +
  geom_col(fill = "coral") +
  labs(x = "Date", y = "Reported cases") +
  ggtitle("Reported measles cases", subtitle = "Niger")
```

We can also look at the data by deparment, as below. Note that the cycles of large outbreaks are less regular than at the national scale (also see [Ferrari et al. 2008, Nature](https://www.nature.com/articles/nature06509)).

```{r plot-region-cases}
ggplot(niger_measles, aes(x = date, y = cases)) +
  geom_col(fill = "coral") +
  labs(x = "Date", y = "Reported cases") +
  facet_wrap(~region, scales = "free_y") +
  theme_minimal(base_size = 8)
```

## Analysis
### Time series decomposition and identifying exceptionally large outbreaks
The first step is to identify "exceptionally large outbreaks." To do this, I will first decompose the time series into trend, seasonal, and random components. I will use the trend component as the time series for identifying large outbreaks as anomalies. An exceptionally large outbreak year will be defined as a year in which the maximum value of the trend component exceeds 4 standard deviations (4$\sigma$) of the mean of the trend component.

Much of this is based on this: https://anomaly.io/anomaly-detection-moving-median-decomposition/.

```{r ts_decomp}
# Convert national cases to time series object
start_date <- min(national_data$date)
end_date <- max(national_data$date)

national_ts <- ts(
  national_data$total_cases, 
  start = c(year(start_date), month(start_date)), 
  end = c(year(end_date), month(end_date)), 
  frequency = 53  # weekly frequency is problematic, but choosing 52 weeks/year
)

# Decompose the time series and calculate anomaly threshold
trend <- runmed(national_ts, k = 53)  # median trend
detrend <- national_ts / trend  # remove the trend assuming multiplicative dynamics
m <- t(matrix(data = detrend, nrow = 53))  # quick matrix of time series
seasonal <- colMeans(m, na.rm = T)  # calculate seasonal component, averaged over weeks
random <- national_ts / (trend * seasonal)  # calculate random component
anom_thresh <- mean(random, na.rm = T) + 4*sd(random, na.rm = T)

# Plot the trend with threshold for outbreak
rand_df <- tibble(
  value = random,
  date = national_data$date[1:length(trend)]
)

ggplot(rand_df, aes(x = date, y = value)) +
  geom_line(aes(color = as.factor(year(date)))) +
  geom_hline(aes(yintercept = anom_thresh), linetype = 2) +
  labs(y = "Random component", x = "Date") +
  scale_color_discrete(name = "Year") +
  annotate("text", 
           x = min(national_data$date)+365, 
           y = 3.25, 
           label = expression(paste("anomaly threshold (4", sigma, ")")), 
           size = 3)
```